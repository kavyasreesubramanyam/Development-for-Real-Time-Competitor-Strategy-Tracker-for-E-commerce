{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3lv8hi_8ug-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pCPHo_g3uWpb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f9a85c1-340a-4dd3-d2bb-146843e91d15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting playwright\n",
            "  Downloading playwright-1.57.0-py3-none-manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (1.6.0)\n",
            "Collecting pyee<14,>=13 (from playwright)\n",
            "  Downloading pyee-13.0.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.12/dist-packages (from playwright) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from pyee<14,>=13->playwright) (4.15.0)\n",
            "Downloading playwright-1.57.0-py3-none-manylinux1_x86_64.whl (46.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.0/46.0 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyee-13.0.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyee, playwright\n",
            "Successfully installed playwright-1.57.0 pyee-13.0.0\n",
            "Downloading Chromium 143.0.7499.4 (playwright build v1200)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1200/chromium-linux.zip\u001b[22m\n",
            "(node:373) [DEP0169] DeprecationWarning: `url.parse()` behavior is not standardized and prone to errors that have security implications. Use the WHATWG URL API instead. CVEs are not issued for `url.parse()` vulnerabilities.\n",
            "(Use `node --trace-deprecation ...` to show where the warning was created)\n",
            "\u001b[1G164.7 MiB [] 0% 0.0s\u001b[0K\u001b[1G164.7 MiB [] 0% 44.5s\u001b[0K\u001b[1G164.7 MiB [] 0% 86.5s\u001b[0K\u001b[1G164.7 MiB [] 0% 26.0s\u001b[0K\u001b[1G164.7 MiB [] 0% 20.0s\u001b[0K\u001b[1G164.7 MiB [] 0% 20.4s\u001b[0K\u001b[1G164.7 MiB [] 0% 15.1s\u001b[0K\u001b[1G164.7 MiB [] 1% 10.5s\u001b[0K\u001b[1G164.7 MiB [] 1% 8.2s\u001b[0K\u001b[1G164.7 MiB [] 1% 7.4s\u001b[0K\u001b[1G164.7 MiB [] 2% 6.5s\u001b[0K\u001b[1G164.7 MiB [] 2% 5.9s\u001b[0K\u001b[1G164.7 MiB [] 3% 5.5s\u001b[0K\u001b[1G164.7 MiB [] 3% 5.2s\u001b[0K\u001b[1G164.7 MiB [] 4% 5.4s\u001b[0K\u001b[1G164.7 MiB [] 4% 5.8s\u001b[0K\u001b[1G164.7 MiB [] 4% 5.5s\u001b[0K\u001b[1G164.7 MiB [] 5% 5.3s\u001b[0K\u001b[1G164.7 MiB [] 6% 5.0s\u001b[0K\u001b[1G164.7 MiB [] 6% 4.7s\u001b[0K\u001b[1G164.7 MiB [] 7% 4.6s\u001b[0K\u001b[1G164.7 MiB [] 7% 4.5s\u001b[0K\u001b[1G164.7 MiB [] 8% 4.3s\u001b[0K\u001b[1G164.7 MiB [] 9% 4.2s\u001b[0K\u001b[1G164.7 MiB [] 9% 4.1s\u001b[0K\u001b[1G164.7 MiB [] 10% 4.0s\u001b[0K\u001b[1G164.7 MiB [] 10% 3.9s\u001b[0K\u001b[1G164.7 MiB [] 11% 3.7s\u001b[0K\u001b[1G164.7 MiB [] 12% 3.6s\u001b[0K\u001b[1G164.7 MiB [] 13% 3.6s\u001b[0K\u001b[1G164.7 MiB [] 14% 3.5s\u001b[0K\u001b[1G164.7 MiB [] 15% 3.6s\u001b[0K\u001b[1G164.7 MiB [] 15% 3.5s\u001b[0K\u001b[1G164.7 MiB [] 16% 3.6s\u001b[0K\u001b[1G164.7 MiB [] 16% 3.5s\u001b[0K\u001b[1G164.7 MiB [] 17% 3.5s\u001b[0K\u001b[1G164.7 MiB [] 17% 3.4s\u001b[0K\u001b[1G164.7 MiB [] 18% 3.3s\u001b[0K\u001b[1G164.7 MiB [] 19% 3.3s\u001b[0K\u001b[1G164.7 MiB [] 20% 3.2s\u001b[0K\u001b[1G164.7 MiB [] 21% 3.1s\u001b[0K\u001b[1G164.7 MiB [] 22% 3.1s\u001b[0K\u001b[1G164.7 MiB [] 23% 3.0s\u001b[0K\u001b[1G164.7 MiB [] 24% 2.9s\u001b[0K\u001b[1G164.7 MiB [] 25% 2.9s\u001b[0K\u001b[1G164.7 MiB [] 25% 2.8s\u001b[0K\u001b[1G164.7 MiB [] 26% 2.8s\u001b[0K\u001b[1G164.7 MiB [] 27% 2.7s\u001b[0K\u001b[1G164.7 MiB [] 28% 2.6s\u001b[0K\u001b[1G164.7 MiB [] 29% 2.6s\u001b[0K\u001b[1G164.7 MiB [] 30% 2.5s\u001b[0K\u001b[1G164.7 MiB [] 31% 2.5s\u001b[0K\u001b[1G164.7 MiB [] 31% 2.4s\u001b[0K\u001b[1G164.7 MiB [] 32% 2.4s\u001b[0K\u001b[1G164.7 MiB [] 33% 2.4s\u001b[0K\u001b[1G164.7 MiB [] 34% 2.4s\u001b[0K\u001b[1G164.7 MiB [] 35% 2.4s\u001b[0K\u001b[1G164.7 MiB [] 36% 2.4s\u001b[0K\u001b[1G164.7 MiB [] 37% 2.4s\u001b[0K\u001b[1G164.7 MiB [] 37% 2.3s\u001b[0K\u001b[1G164.7 MiB [] 38% 2.3s\u001b[0K\u001b[1G164.7 MiB [] 39% 2.3s\u001b[0K\u001b[1G164.7 MiB [] 39% 2.2s\u001b[0K\u001b[1G164.7 MiB [] 40% 2.2s\u001b[0K\u001b[1G164.7 MiB [] 41% 2.1s\u001b[0K\u001b[1G164.7 MiB [] 42% 2.1s\u001b[0K\u001b[1G164.7 MiB [] 43% 2.0s\u001b[0K\u001b[1G164.7 MiB [] 44% 2.0s\u001b[0K\u001b[1G164.7 MiB [] 45% 1.9s\u001b[0K\u001b[1G164.7 MiB [] 46% 1.9s\u001b[0K\u001b[1G164.7 MiB [] 47% 1.9s\u001b[0K\u001b[1G164.7 MiB [] 48% 1.8s\u001b[0K\u001b[1G164.7 MiB [] 49% 1.8s\u001b[0K\u001b[1G164.7 MiB [] 50% 1.7s\u001b[0K\u001b[1G164.7 MiB [] 51% 1.7s\u001b[0K\u001b[1G164.7 MiB [] 52% 1.7s\u001b[0K\u001b[1G164.7 MiB [] 52% 1.6s\u001b[0K\u001b[1G164.7 MiB [] 53% 1.6s\u001b[0K\u001b[1G164.7 MiB [] 54% 1.6s\u001b[0K\u001b[1G164.7 MiB [] 55% 1.5s\u001b[0K\u001b[1G164.7 MiB [] 56% 1.5s\u001b[0K\u001b[1G164.7 MiB [] 57% 1.4s\u001b[0K\u001b[1G164.7 MiB [] 58% 1.4s\u001b[0K\u001b[1G164.7 MiB [] 59% 1.4s\u001b[0K\u001b[1G164.7 MiB [] 59% 1.3s\u001b[0K\u001b[1G164.7 MiB [] 60% 1.3s\u001b[0K\u001b[1G164.7 MiB [] 61% 1.3s\u001b[0K\u001b[1G164.7 MiB [] 62% 1.3s\u001b[0K\u001b[1G164.7 MiB [] 62% 1.2s\u001b[0K\u001b[1G164.7 MiB [] 63% 1.2s\u001b[0K\u001b[1G164.7 MiB [] 64% 1.2s\u001b[0K\u001b[1G164.7 MiB [] 65% 1.2s\u001b[0K\u001b[1G164.7 MiB [] 65% 1.1s\u001b[0K\u001b[1G164.7 MiB [] 66% 1.1s\u001b[0K\u001b[1G164.7 MiB [] 67% 1.1s\u001b[0K\u001b[1G164.7 MiB [] 68% 1.0s\u001b[0K\u001b[1G164.7 MiB [] 69% 1.0s\u001b[0K\u001b[1G164.7 MiB [] 70% 1.0s\u001b[0K\u001b[1G164.7 MiB [] 71% 0.9s\u001b[0K\u001b[1G164.7 MiB [] 72% 0.9s\u001b[0K\u001b[1G164.7 MiB [] 73% 0.9s\u001b[0K\u001b[1G164.7 MiB [] 74% 0.8s\u001b[0K\u001b[1G164.7 MiB [] 75% 0.8s\u001b[0K\u001b[1G164.7 MiB [] 76% 0.7s\u001b[0K\u001b[1G164.7 MiB [] 77% 0.7s\u001b[0K\u001b[1G164.7 MiB [] 78% 0.7s\u001b[0K\u001b[1G164.7 MiB [] 79% 0.6s\u001b[0K\u001b[1G164.7 MiB [] 80% 0.6s\u001b[0K\u001b[1G164.7 MiB [] 81% 0.6s\u001b[0K\u001b[1G164.7 MiB [] 82% 0.5s\u001b[0K\u001b[1G164.7 MiB [] 83% 0.5s\u001b[0K\u001b[1G164.7 MiB [] 84% 0.5s\u001b[0K\u001b[1G164.7 MiB [] 85% 0.4s\u001b[0K\u001b[1G164.7 MiB [] 86% 0.4s\u001b[0K\u001b[1G164.7 MiB [] 87% 0.4s\u001b[0K\u001b[1G164.7 MiB [] 88% 0.4s\u001b[0K\u001b[1G164.7 MiB [] 88% 0.3s\u001b[0K\u001b[1G164.7 MiB [] 89% 0.3s\u001b[0K\u001b[1G164.7 MiB [] 90% 0.3s\u001b[0K\u001b[1G164.7 MiB [] 91% 0.3s\u001b[0K\u001b[1G164.7 MiB [] 92% 0.2s\u001b[0K\u001b[1G164.7 MiB [] 93% 0.2s\u001b[0K\u001b[1G164.7 MiB [] 94% 0.2s\u001b[0K\u001b[1G164.7 MiB [] 95% 0.1s\u001b[0K\u001b[1G164.7 MiB [] 96% 0.1s\u001b[0K\u001b[1G164.7 MiB [] 97% 0.1s\u001b[0K\u001b[1G164.7 MiB [] 98% 0.1s\u001b[0K\u001b[1G164.7 MiB [] 98% 0.0s\u001b[0K\u001b[1G164.7 MiB [] 99% 0.0s\u001b[0K\u001b[1G164.7 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium 143.0.7499.4 (playwright build v1200) downloaded to /root/.cache/ms-playwright/chromium-1200\n",
            "Downloading FFMPEG playwright build v1011\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/ffmpeg/1011/ffmpeg-linux.zip\u001b[22m\n",
            "(node:474) [DEP0169] DeprecationWarning: `url.parse()` behavior is not standardized and prone to errors that have security implications. Use the WHATWG URL API instead. CVEs are not issued for `url.parse()` vulnerabilities.\n",
            "(Use `node --trace-deprecation ...` to show where the warning was created)\n",
            "\u001b[1G2.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 13% 0.1s\u001b[0K\u001b[1G2.3 MiB [] 46% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 82% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 100% 0.0s\u001b[0K\n",
            "FFMPEG playwright build v1011 downloaded to /root/.cache/ms-playwright/ffmpeg-1011\n",
            "Downloading Chromium Headless Shell 143.0.7499.4 (playwright build v1200)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1200/chromium-headless-shell-linux.zip\u001b[22m\n",
            "(node:485) [DEP0169] DeprecationWarning: `url.parse()` behavior is not standardized and prone to errors that have security implications. Use the WHATWG URL API instead. CVEs are not issued for `url.parse()` vulnerabilities.\n",
            "(Use `node --trace-deprecation ...` to show where the warning was created)\n",
            "\u001b[1G109.7 MiB [] 0% 0.0s\u001b[0K\u001b[1G109.7 MiB [] 0% 40.7s\u001b[0K\u001b[1G109.7 MiB [] 0% 17.8s\u001b[0K\u001b[1G109.7 MiB [] 0% 12.4s\u001b[0K\u001b[1G109.7 MiB [] 0% 7.9s\u001b[0K\u001b[1G109.7 MiB [] 1% 4.6s\u001b[0K\u001b[1G109.7 MiB [] 3% 3.3s\u001b[0K\u001b[1G109.7 MiB [] 4% 2.6s\u001b[0K\u001b[1G109.7 MiB [] 5% 2.3s\u001b[0K\u001b[1G109.7 MiB [] 5% 2.5s\u001b[0K\u001b[1G109.7 MiB [] 6% 2.5s\u001b[0K\u001b[1G109.7 MiB [] 7% 2.2s\u001b[0K\u001b[1G109.7 MiB [] 9% 2.0s\u001b[0K\u001b[1G109.7 MiB [] 10% 1.9s\u001b[0K\u001b[1G109.7 MiB [] 11% 1.8s\u001b[0K\u001b[1G109.7 MiB [] 12% 1.8s\u001b[0K\u001b[1G109.7 MiB [] 13% 1.7s\u001b[0K\u001b[1G109.7 MiB [] 14% 1.7s\u001b[0K\u001b[1G109.7 MiB [] 15% 1.6s\u001b[0K\u001b[1G109.7 MiB [] 15% 1.8s\u001b[0K\u001b[1G109.7 MiB [] 16% 1.8s\u001b[0K\u001b[1G109.7 MiB [] 17% 1.8s\u001b[0K\u001b[1G109.7 MiB [] 18% 1.8s\u001b[0K\u001b[1G109.7 MiB [] 19% 1.7s\u001b[0K\u001b[1G109.7 MiB [] 20% 1.6s\u001b[0K\u001b[1G109.7 MiB [] 21% 1.6s\u001b[0K\u001b[1G109.7 MiB [] 22% 1.6s\u001b[0K\u001b[1G109.7 MiB [] 23% 1.6s\u001b[0K\u001b[1G109.7 MiB [] 24% 1.6s\u001b[0K\u001b[1G109.7 MiB [] 25% 1.5s\u001b[0K\u001b[1G109.7 MiB [] 27% 1.4s\u001b[0K\u001b[1G109.7 MiB [] 28% 1.4s\u001b[0K\u001b[1G109.7 MiB [] 29% 1.4s\u001b[0K\u001b[1G109.7 MiB [] 30% 1.3s\u001b[0K\u001b[1G109.7 MiB [] 32% 1.3s\u001b[0K\u001b[1G109.7 MiB [] 33% 1.2s\u001b[0K\u001b[1G109.7 MiB [] 35% 1.2s\u001b[0K\u001b[1G109.7 MiB [] 37% 1.1s\u001b[0K\u001b[1G109.7 MiB [] 38% 1.1s\u001b[0K\u001b[1G109.7 MiB [] 40% 1.0s\u001b[0K\u001b[1G109.7 MiB [] 41% 1.0s\u001b[0K\u001b[1G109.7 MiB [] 43% 0.9s\u001b[0K\u001b[1G109.7 MiB [] 45% 0.9s\u001b[0K\u001b[1G109.7 MiB [] 47% 0.8s\u001b[0K\u001b[1G109.7 MiB [] 48% 0.8s\u001b[0K\u001b[1G109.7 MiB [] 50% 0.8s\u001b[0K\u001b[1G109.7 MiB [] 52% 0.7s\u001b[0K\u001b[1G109.7 MiB [] 54% 0.7s\u001b[0K\u001b[1G109.7 MiB [] 55% 0.6s\u001b[0K\u001b[1G109.7 MiB [] 57% 0.6s\u001b[0K\u001b[1G109.7 MiB [] 59% 0.6s\u001b[0K\u001b[1G109.7 MiB [] 61% 0.6s\u001b[0K\u001b[1G109.7 MiB [] 62% 0.5s\u001b[0K\u001b[1G109.7 MiB [] 64% 0.5s\u001b[0K\u001b[1G109.7 MiB [] 65% 0.5s\u001b[0K\u001b[1G109.7 MiB [] 67% 0.4s\u001b[0K\u001b[1G109.7 MiB [] 68% 0.4s\u001b[0K\u001b[1G109.7 MiB [] 69% 0.4s\u001b[0K\u001b[1G109.7 MiB [] 70% 0.4s\u001b[0K\u001b[1G109.7 MiB [] 71% 0.4s\u001b[0K\u001b[1G109.7 MiB [] 72% 0.4s\u001b[0K\u001b[1G109.7 MiB [] 74% 0.4s\u001b[0K\u001b[1G109.7 MiB [] 75% 0.3s\u001b[0K\u001b[1G109.7 MiB [] 77% 0.3s\u001b[0K\u001b[1G109.7 MiB [] 80% 0.3s\u001b[0K\u001b[1G109.7 MiB [] 81% 0.2s\u001b[0K\u001b[1G109.7 MiB [] 83% 0.2s\u001b[0K\u001b[1G109.7 MiB [] 85% 0.2s\u001b[0K\u001b[1G109.7 MiB [] 87% 0.2s\u001b[0K\u001b[1G109.7 MiB [] 88% 0.1s\u001b[0K\u001b[1G109.7 MiB [] 90% 0.1s\u001b[0K\u001b[1G109.7 MiB [] 91% 0.1s\u001b[0K\u001b[1G109.7 MiB [] 93% 0.1s\u001b[0K\u001b[1G109.7 MiB [] 95% 0.1s\u001b[0K\u001b[1G109.7 MiB [] 97% 0.0s\u001b[0K\u001b[1G109.7 MiB [] 98% 0.0s\u001b[0K\u001b[1G109.7 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium Headless Shell 143.0.7499.4 (playwright build v1200) downloaded to /root/.cache/ms-playwright/chromium_headless_shell-1200\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  at-spi2-core gsettings-desktop-schemas libatk1.0-data libxtst6\n",
            "  session-migration\n",
            "The following NEW packages will be installed:\n",
            "  at-spi2-core gsettings-desktop-schemas libatk-bridge2.0-0 libatk1.0-0\n",
            "  libatk1.0-data libatspi2.0-0 libxcomposite1 libxtst6 session-migration\n",
            "0 upgraded, 9 newly installed, 0 to remove and 1 not upgraded.\n",
            "Need to get 318 kB of archives.\n",
            "After this operation, 1,497 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatspi2.0-0 amd64 2.44.0-3 [80.9 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 session-migration amd64 0.3.6 [9,774 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 gsettings-desktop-schemas all 42.0-1ubuntu1 [31.1 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 at-spi2-core amd64 2.44.0-3 [54.4 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk1.0-data all 2.36.0-3build1 [2,824 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk1.0-0 amd64 2.36.0-3build1 [51.9 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-bridge2.0-0 amd64 2.38.0-3 [66.6 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcomposite1 amd64 1:0.4.5-1build2 [7,192 B]\n",
            "Fetched 318 kB in 1s (590 kB/s)\n",
            "Selecting previously unselected package libatspi2.0-0:amd64.\n",
            "(Reading database ... 117528 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libatspi2.0-0_2.44.0-3_amd64.deb ...\n",
            "Unpacking libatspi2.0-0:amd64 (2.44.0-3) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../1-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package session-migration.\n",
            "Preparing to unpack .../2-session-migration_0.3.6_amd64.deb ...\n",
            "Unpacking session-migration (0.3.6) ...\n",
            "Selecting previously unselected package gsettings-desktop-schemas.\n",
            "Preparing to unpack .../3-gsettings-desktop-schemas_42.0-1ubuntu1_all.deb ...\n",
            "Unpacking gsettings-desktop-schemas (42.0-1ubuntu1) ...\n",
            "Selecting previously unselected package at-spi2-core.\n",
            "Preparing to unpack .../4-at-spi2-core_2.44.0-3_amd64.deb ...\n",
            "Unpacking at-spi2-core (2.44.0-3) ...\n",
            "Selecting previously unselected package libatk1.0-data.\n",
            "Preparing to unpack .../5-libatk1.0-data_2.36.0-3build1_all.deb ...\n",
            "Unpacking libatk1.0-data (2.36.0-3build1) ...\n",
            "Selecting previously unselected package libatk1.0-0:amd64.\n",
            "Preparing to unpack .../6-libatk1.0-0_2.36.0-3build1_amd64.deb ...\n",
            "Unpacking libatk1.0-0:amd64 (2.36.0-3build1) ...\n",
            "Selecting previously unselected package libatk-bridge2.0-0:amd64.\n",
            "Preparing to unpack .../7-libatk-bridge2.0-0_2.38.0-3_amd64.deb ...\n",
            "Unpacking libatk-bridge2.0-0:amd64 (2.38.0-3) ...\n",
            "Selecting previously unselected package libxcomposite1:amd64.\n",
            "Preparing to unpack .../8-libxcomposite1_1%3a0.4.5-1build2_amd64.deb ...\n",
            "Unpacking libxcomposite1:amd64 (1:0.4.5-1build2) ...\n",
            "Setting up session-migration (0.3.6) ...\n",
            "Created symlink /etc/systemd/user/graphical-session-pre.target.wants/session-migration.service ‚Üí /usr/lib/systemd/user/session-migration.service.\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up libatspi2.0-0:amd64 (2.44.0-3) ...\n",
            "Setting up libatk1.0-data (2.36.0-3build1) ...\n",
            "Setting up libatk1.0-0:amd64 (2.36.0-3build1) ...\n",
            "Setting up libxcomposite1:amd64 (1:0.4.5-1build2) ...\n",
            "Setting up gsettings-desktop-schemas (42.0-1ubuntu1) ...\n",
            "Setting up libatk-bridge2.0-0:amd64 (2.38.0-3) ...\n",
            "Processing triggers for libglib2.0-0:amd64 (2.72.4-0ubuntu2.6) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.11) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Setting up at-spi2-core (2.44.0-3) ...\n",
            "Scraping Page 1 ‚Üí https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=1\n",
            "Scraping Page 2 ‚Üí https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=2\n",
            "Scraping Page 3 ‚Üí https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=3\n",
            "Scraping Page 4 ‚Üí https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=4\n",
            "Scraping Page 5 ‚Üí https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=5\n",
            "Scraping Page 6 ‚Üí https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=6\n",
            "Scraping Page 7 ‚Üí https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=7\n",
            "Scraping Page 8 ‚Üí https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=8\n",
            "Scraping Page 9 ‚Üí https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=9\n",
            "Scraping Page 10 ‚Üí https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=10\n",
            "Scraping Page 11 ‚Üí https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=11\n",
            "Scraping Page 12 ‚Üí https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=12\n",
            "Scraping Page 13 ‚Üí https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=13\n",
            "Scraping Page 14 ‚Üí https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=14\n",
            "Scraping Page 15 ‚Üí https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=15\n",
            "Scraping Page 16 ‚Üí https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=16\n",
            "Scraping Page 17 ‚Üí https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=17\n",
            "Scraping Page 18 ‚Üí https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=18\n",
            "Scraping Page 19 ‚Üí https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=19\n",
            "Scraping Page 20 ‚Üí https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=20\n",
            "Scraping Page 21 ‚Üí https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=21\n",
            " No more pages left. Stopping.\n",
            " Collected 117 total products\n",
            "Saved CSV ‚Üí ioutput/products_all_ajax.csv\n",
            "Saved JSON ‚Üí ioutput/products_all_ajax.json\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install playwright nest_asyncio\n",
        "!playwright install chromium\n",
        "!apt-get install libatk1.0-0 libatk-bridge2.0-0 libatspi2.0-0 libxcomposite1\n",
        "\n",
        "\n",
        "import asyncio, json, csv\n",
        "from pathlib import Path\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "from playwright.async_api import async_playwright\n",
        "\n",
        "\n",
        "\n",
        "#  3. BASE URL\n",
        "BASE_URL = \"https://webscraper.io/test-sites/e-commerce/static/computers/laptops\"\n",
        "\n",
        "# 4. MAIN SCRAPING FUNCTION\n",
        "async def scrape_ajax_site():\n",
        "\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True)   # Launch browser\n",
        "        ctx = await browser.new_context()                  # New browser session\n",
        "        page = await ctx.new_page()                        # New tab\n",
        "\n",
        "        rows = []                                         # Stores ALL products\n",
        "        page_no = 1                                       #  Start from page 1\n",
        "\n",
        "        # 5. PAGE NUMBER LOOP (1 ‚Üí 20)\n",
        "        while True:\n",
        "            url = f\"{BASE_URL}?page={page_no}\"             #  Build page URL\n",
        "            print(f\"Scraping Page {page_no} ‚Üí {url}\")\n",
        "            await page.goto(url, timeout=60000)            #  Open that page\n",
        "            try:\n",
        "                await page.wait_for_selector(\".thumbnail\", timeout=10000)\n",
        "                #  Wait for product cards\n",
        "            except:\n",
        "                print(\" No more pages left. Stopping.\")\n",
        "                break                                     #  Stop when no products found\n",
        "            cards = await page.query_selector_all(\".thumbnail\")\n",
        "\n",
        "            if not cards:                                 # Safety stop\n",
        "                print(\" Last page reached.\")\n",
        "                break\n",
        "            #  Extract products from the CURRENT page\n",
        "            for card in cards:\n",
        "\n",
        "                title_el = await card.query_selector(\".title\")\n",
        "                title = (await title_el.text_content()).strip() if title_el else None\n",
        "\n",
        "                url = await title_el.get_attribute(\"href\") if title_el else None\n",
        "\n",
        "                price_el = await card.query_selector(\".price\")\n",
        "                price = (await price_el.text_content()).strip() if price_el else None\n",
        "\n",
        "                stars = await card.query_selector_all(\".ratings .glyphicon-star\")\n",
        "                rating = len(stars) if stars else 0\n",
        "\n",
        "                img_el = await card.query_selector(\"img\")\n",
        "                img_src = await img_el.get_attribute(\"src\") if img_el else None\n",
        "\n",
        "                rows.append({\n",
        "                    \"title\": title,\n",
        "                    \"price\": price,\n",
        "                    \"rating_stars\": rating,\n",
        "                    \"product_url\": url,\n",
        "                    \"image_url\": img_src,\n",
        "                    \"page_no\": page_no\n",
        "                })\n",
        "\n",
        "            page_no += 1                                   # Go to next page number\n",
        "\n",
        "        await browser.close()\n",
        "        return rows\n",
        "\n",
        "#  6. RUN SCRAPER\n",
        "\n",
        "data = asyncio.get_event_loop().run_until_complete(scrape_ajax_site())\n",
        "print(f\" Collected {len(data)} total products\")\n",
        "\n",
        "\n",
        "# 7. SAVE OUTPUT FILES\n",
        "\n",
        "Path(\"ioutput\").mkdir(exist_ok=True)                       #  Create output folder\n",
        "\n",
        "csv_path = Path(\"ioutput/products_all_ajax.csv\")\n",
        "json_path = Path(\"ioutput/products_all_ajax.json\")\n",
        "\n",
        "# Save CSV\n",
        "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=data[0].keys())\n",
        "    writer.writeheader()\n",
        "    writer.writerows(data)\n",
        "\n",
        "# Save JSON\n",
        "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"Saved CSV ‚Üí {csv_path}\")\n",
        "print(f\"Saved JSON ‚Üí {json_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìå Observations ‚Äì Page-Based Pagination Web Scraping\n",
        "\n",
        "üîπ 1. Pagination Handling\n",
        "\n",
        "The scraper uses URL-based pagination by incrementing the page query parameter (?page=1, ?page=2, ‚Ä¶).\n",
        "\n",
        "This confirms that the target website supports static server-side pagination, not infinite scroll or AJAX-only loading.\n",
        "\n",
        "Scraping stops automatically when:\n",
        "\n",
        "No product cards (.thumbnail) are found, or\n",
        "\n",
        "The selector wait times out.\n",
        "\n",
        "Observation:\n",
        "This method is reliable and deterministic because each page has a fixed URL and complete HTML content.\n",
        "\n",
        "\n",
        "üîπ 2. Scraping Strategy\n",
        "\n",
        "The script opens one browser session and one page (tab).\n",
        "\n",
        "The same page object is reused for all page numbers.\n",
        "\n",
        "For each page:\n",
        "\n",
        "All product cards are selected.\n",
        "\n",
        "Product details are extracted from the listing page itself, without visiting individual product pages.\n",
        "\n",
        "Observation:\n",
        "This approach is efficient and avoids unnecessary navigation, reducing scraping time and browser overhead.\n",
        "\n",
        "üîπ 3. Extracted Data Fields\n",
        "\n",
        "For every product, the following information is collected:\n",
        "\n",
        "Title ‚Äì Product name\n",
        "\n",
        "Price ‚Äì Displayed product price\n",
        "\n",
        "Rating (Stars) ‚Äì Derived by counting star icons\n",
        "\n",
        "Product URL ‚Äì Link to product detail page\n",
        "\n",
        "Image URL ‚Äì Product image source\n",
        "\n",
        "Page Number ‚Äì Page from which the product was scraped\n",
        "\n",
        "Observation:\n",
        "The dataset is well-structured and suitable for further analysis such as price comparison, rating analysis, or visualization.\n",
        "\n",
        "üîπ 4. Rating Extraction Logic\n",
        "\n",
        "Product ratings are calculated by counting the number of .glyphicon-star elements.\n",
        "\n",
        "This avoids reliance on text values and ensures consistent numeric ratings.\n",
        "\n",
        "Observation:\n",
        "Using DOM element counts for ratings is robust and less error-prone than parsing text.\n",
        "\n",
        "üîπ 5. Loop Termination Condition\n",
        "\n",
        "The scraper safely exits when:\n",
        "\n",
        "The selector .thumbnail is not found, or\n",
        "\n",
        "An empty product list is returned.\n",
        "\n",
        "Observation:\n",
        "This prevents infinite loops and ensures graceful termination when the last page is reached.\n",
        "\n",
        "üîπ 6. Performance Considerations\n",
        "\n",
        "Page-based navigation is faster than AJAX ‚ÄúLoad More‚Äù approaches.\n",
        "\n",
        "No JavaScript event handling or network-idle polling is required.\n",
        "\n",
        "Observation:\n",
        "This method scales well for large datasets and is ideal for production-grade scraping.\n",
        "\n",
        "üîπ 7. Output Generation\n",
        "\n",
        "Data is saved in both CSV and JSON formats.\n",
        "\n",
        "Output directory is created safely using pathlib.\n",
        "\n",
        "Observation:\n",
        "Providing both formats increases usability for data analysis, machine learning, and reporting.\n",
        "\n",
        "üîπ 8. Reliability & Maintainability\n",
        "\n",
        "The scraper logic is simple, readable, and modular.\n",
        "\n",
        "Fewer failure points compared to dynamic/AJAX scraping.\n",
        "\n",
        "Observation:\n",
        "This approach is highly maintainable and suitable for academic projects, internships, and interviews.\n",
        "\n",
        "‚úÖ Final Conclusion\n",
        "\n",
        "This scraper successfully demonstrates page-based pagination scraping, efficiently collecting structured product data from a static e-commerce website. The approach is reliable, fast, and well-suited for large-scale data collection, making it preferable over dynamic ‚ÄúLoad More‚Äù scraping when URL-based pagination is available."
      ],
      "metadata": {
        "id": "a1jLY_sfi1-u"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hFz4XqxRiyQD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}